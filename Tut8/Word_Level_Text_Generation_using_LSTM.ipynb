{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilfierd/ATI-tut/blob/main/Tut8/Word_Level_Text_Generation_using_LSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ZVeEoXuvnfZ"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fgVRgknvqXi"
      },
      "source": [
        "One of the fundamental tasks in NLP is Word level text generation, which is to predict the next few words that people most likely anticipate as they type along.\n",
        "\n",
        "The following diagram illustrates what we are trying to address. What could be the next word? We will build a neural model to predict it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96WVxx-6xidT"
      },
      "source": [
        "![image-411.webp](data:image/webp;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c75SsoeUyQto"
      },
      "source": [
        "We can use RNNs for word-level text generation tasks. However, this task requires a longer memory of which words are important. Therefore, in this tutorial, we will build this language model using the LSTM architecture.\n",
        "\n",
        "**LSTM (Long Short-Term Memory) networks** are a special type of RNN with gates that control what information to remember, forget, or pass along. This allows LSTMs to better handle long-term dependencies in sequences, making them ideal for text generation where context from earlier words matters.\n",
        "\n",
        "We will proceed as follows:\n",
        "\n",
        "1. Create a text corpus\n",
        "2. Prepare training data\n",
        "3. Specify, compile, and fit the LSTM model\n",
        "4. Test the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye3OOVDhxraT"
      },
      "source": [
        "# Text corpus\n",
        "\n",
        "First, we will prepare a text corpus. This includes developing a vocabulary and creating a dictionary for mapping words to integers and vice versa. In this tutorial, we will use the text corpus from \"Truyện Kiều.\"\n",
        "\n",
        "The text corpus can be downloaded from the following link:\n",
        "\n",
        "[https://drive.google.com/file/d/1fpVcAHsXnj7XAE17frTBqwdpm6CWZjrm/view?usp=sharing](https://drive.google.com/file/d/1fpVcAHsXnj7XAE17frTBqwdpm6CWZjrm/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Wy0aZPd2LXJ"
      },
      "source": [
        "Firstly we will load the text corpus into a Pandax Dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-HMznUIyL1th"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Required Libraries\n",
        "\n",
        "We'll start by importing the essential libraries:\n",
        "- **NumPy**: For numerical operations and array manipulation\n",
        "- **Pandas**: For loading and processing the text corpus from CSV files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "m3wd6oOv2Pxx",
        "outputId": "5eff338a-9583-4288-dc33-b4efcce621c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3258 entries, 0 to 3257\n",
            "Data columns (total 1 columns):\n",
            " #   Column  Non-Null Count  Dtype \n",
            "---  ------  --------------  ----- \n",
            " 0   row     3258 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 25.6+ KB\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"data\",\n  \"rows\": 3258,\n  \"fields\": [\n    {\n      \"column\": \"row\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3255,\n        \"samples\": [\n          \"2350. N\\u1eeda ph\\u1ea7n khi\\u1ebfp s\\u1ee3, n\\u1eeda ph\\u1ea7n m\\u1eebng vui .\",\n          \"135.. Tr\\u00f4ng ch\\u1eebng th\\u1ea5y m\\u1ed9t v\\u0103n nh\\u00e2n,\",\n          \"Nghi\\u00eam qu\\u00e2n tuy\\u1ec3n t\\u01b0\\u1edbng s\\u1eb5n s\\u00e0ng,\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "data"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-1373a322-57bf-42b3-9827-391f540af08e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1..Trăm năm trong cõi người ta,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2..Chữ tài chữ mệnh khéo là ghét nhau.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3..Trải qua một cuộc bể dâu,</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4..Những điều trông thấy mà đau đớn lòng.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.. Lạ gì bỉ sắc tư phong,</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1373a322-57bf-42b3-9827-391f540af08e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-1373a322-57bf-42b3-9827-391f540af08e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-1373a322-57bf-42b3-9827-391f540af08e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-eff809cf-e8b5-4918-b33d-ef2fc007950b\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-eff809cf-e8b5-4918-b33d-ef2fc007950b')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-eff809cf-e8b5-4918-b33d-ef2fc007950b button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "                                         row\n",
              "0            1..Trăm năm trong cõi người ta,\n",
              "1     2..Chữ tài chữ mệnh khéo là ghét nhau.\n",
              "2               3..Trải qua một cuộc bể dâu,\n",
              "3  4..Những điều trông thấy mà đau đớn lòng.\n",
              "4                 5.. Lạ gì bỉ sắc tư phong,"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = pd.read_csv(\"truyenkieu.txt\",sep=\"/\", names=[\"row\"])\n",
        "data.info()\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Loading the Text Corpus\n",
        "\n",
        "We load the \"Truyện Kiều\" text into a Pandas DataFrame:\n",
        "- Using `/` as separator since the file has a specific format\n",
        "- Naming the column \"row\" for easy reference\n",
        "- `.info()` and `.head()` help us understand the data structure and preview the content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGEoGD4u2fnr"
      },
      "source": [
        "We will perform simple text processing for the sentence in each row:\n",
        "\n",
        "* Remove numbers at the begining of the sentence\n",
        "* Remove period, comma, question mark at the end of sentence\n",
        "* Delete all periods, commas, semicolon, exclamation points, ... in the sentence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Z4RzBwFh18z8"
      },
      "outputs": [],
      "source": [
        "import regex as re\n",
        "\n",
        "def row_transform(row):\n",
        "    # Delete numbers at the beginning of the sentence\n",
        "    row = re.sub(r\"^[0-9\\.]+\", \"\", row)\n",
        "\n",
        "    # Remove period, comma, question mark at the end of sentence\n",
        "    row = re.sub(r\"[\\.,\\?]+$\", \"\", row)\n",
        "\n",
        "    # Delete all periods, commas, semicolons, exclamation points, ... in the sentence\n",
        "    row = row.replace(\",\", \" \").replace(\".\", \" \").replace(\";\", \" \").replace(\"“\", \" \") \\\n",
        "        .replace(\":\", \" \").replace(\"”\", \" \").replace('\"', \" \").replace(\"'\", \" \") \\\n",
        "        .replace(\"!\", \" \").replace(\"?\", \" \")\n",
        "\n",
        "    row = row.strip()\n",
        "    return row"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Preprocessing Function\n",
        "\n",
        "**Why preprocess text?**\n",
        "Text data contains noise (punctuation, numbers, extra spaces) that can confuse the model. Clean text helps the model focus on learning word relationships rather than memorizing formatting.\n",
        "\n",
        "**What this function does:**\n",
        "1. Removes leading numbers (line numbers from the original text)\n",
        "2. Removes trailing punctuation (., ?, ,)\n",
        "3. Removes all punctuation within sentences\n",
        "4. Strips extra whitespace\n",
        "\n",
        "This creates clean, uniform text that's easier for the LSTM to learn from."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnpdaWrs1919",
        "outputId": "5eac1170-6f36-4afc-ee0c-ecb3aefb481e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Length of the corpus: 3258\n",
            "['Trăm năm trong cõi người ta', 'Chữ tài chữ mệnh khéo là ghét nhau', 'Trải qua một cuộc bể dâu', 'Những điều trông thấy mà đau đớn lòng', 'Lạ gì bỉ sắc tư phong']\n"
          ]
        }
      ],
      "source": [
        "data['row'] = data.row.apply(row_transform)\n",
        "textCorpus = data.row.tolist()\n",
        "print(f'Length of the corpus: {len(textCorpus)}')\n",
        "print(textCorpus[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Apply Preprocessing\n",
        "\n",
        "We apply the cleaning function to all rows and convert to a list:\n",
        "- `apply()` processes each sentence through our transformation\n",
        "- `tolist()` converts DataFrame to Python list for easier manipulation\n",
        "- The result is our clean text corpus ready for tokenization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljhtp8CS2_A2"
      },
      "source": [
        "Now we create a vocabulary using the TensorFlow `Tokenizer`.\n",
        "\n",
        "The `Tokenizer` converts text to numbers by:\n",
        "- **Splitting** sentences into words\n",
        "- **Building a vocabulary** that maps each unique word to a unique integer\n",
        "- **Converting** between words and integers\n",
        "\n",
        "**Why tokenization?** Neural networks work with numbers, not text. Each word gets assigned an index (e.g., \"một\"→1, \"đã\"→2). This allows the model to process and learn from the text data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiUkbUV4MkCe",
        "outputId": "9f0302d7-82ee-4c28-eab9-4c320ac54d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Total words in the corpus: 2394\n",
            "{1: 'một', 2: 'đã', 3: 'người', 4: 'nàng', 5: 'lòng', 6: 'lời', 7: 'cho', 8: 'là', 9: 'cũng', 10: 'có', 11: 'rằng', 12: 'ra', 13: 'lại', 14: 'hoa', 15: 'tình', 16: 'mới', 17: 'còn', 18: 'đâu', 19: 'ai', 20: 'chẳng', 21: 'mà', 22: 'thì', 23: 'mình', 24: 'biết', 25: 'này', 26: 'trong', 27: 'đến', 28: 'đường', 29: 'nhà', 30: 'càng', 31: 'nào', 32: 'trời', 33: 'ngày', 34: 'thân', 35: 'như', 36: 'khi', 37: 'vào', 38: 'mặt', 39: 'sao', 40: 'vàng', 41: 'duyên', 42: 'xa', 43: 'về', 44: 'tay', 45: 'sinh', 46: 'làm', 47: 'chàng', 48: 'thôi', 49: 'thấy', 50: 'trước', 51: 'chi', 52: 'nghe', 53: 'những', 54: 'sau', 55: 'hai', 56: 'nỗi', 57: 'từ', 58: 'nước', 59: 'hương', 60: 'nói', 61: 'trông', 62: 'xuân', 63: 'hồng', 64: 'phải', 65: 'ta', 66: 'con', 67: 'thương', 68: 'gió', 69: 'đây', 70: 'thế', 71: 'tiếng', 72: 'chưa', 73: 'mấy', 74: 'tơ', 75: 'ở', 76: 'năm', 77: 'với', 78: 'nghĩ', 79: 'chút', 80: 'gì', 81: 'xưa', 82: 'nhau', 83: 'đi', 84: 'bên', 85: 'giờ', 86: 'mây', 87: 'công', 88: 'được', 89: 'ấy', 90: 'bao', 91: 'hay', 92: 'phận', 93: 'đầu', 94: 'điều', 95: 'trăng', 96: 'ngoài', 97: 'bấy', 98: 'hỏi', 99: 'xanh', 100: 'riêng', 101: 'bóng', 102: 'cửa', 103: 'vừa', 104: 'không', 105: 'lấy', 106: 'bạc', 107: 'cao', 108: 'nơi', 109: 'cùng', 110: 'vội', 111: 'lần', 112: 'đà', 113: 'đời', 114: 'mai', 115: 'gần', 116: 'đào', 117: 'sự', 118: 'tài', 119: 'phong', 120: 'cầm', 121: 'nên', 122: 'bước', 123: 'trên', 124: 'mưa', 125: 'tin', 126: 'chữ', 127: 'non', 128: 'ba', 129: 'vì', 130: 'để', 131: 'gia', 132: 'nhân', 133: 'tìm', 134: 'thay', 135: 'chung', 136: 'dưới', 137: 'quân', 138: 'trần', 139: 'rồi', 140: 'trăm', 141: 'thanh', 142: 'tôi', 143: 'quan', 144: 'chơi', 145: 'châu', 146: 'gặp', 147: 'dễ', 148: 'nhớ', 149: 'kia', 150: 'thư', 151: 'tường', 152: 'kiếp', 153: 'song', 154: 'lâu', 155: 'kiều', 156: 'mười', 157: 'ngọc', 158: 'lên', 159: 'theo', 160: 'ngọn', 161: 'nửa', 162: 'tiên', 163: 'đàn', 164: 'thưa', 165: 'tan', 166: 'đêm', 167: 'hết', 168: 'lạ', 169: 'xem', 170: 'cười', 171: 'đưa', 172: 'bài', 173: 'đồng', 174: 'nay', 175: 'hãy', 176: 'đôi', 177: 'xót', 178: 'cành', 179: 'bà', 180: 'kẻ', 181: 'đem', 182: 'chén', 183: 'oan', 184: 'bể', 185: 'đầy', 186: 'phần', 187: 'đành', 188: 'tiền', 189: 'chăng', 190: 'dầu', 191: 'sầu', 192: 'trường', 193: 'liều', 194: 'qua', 195: 'sẵn', 196: 'tiểu', 197: 'xuống', 198: 'dám', 199: 'nghìn', 200: 'ân', 201: 'dây', 202: 'ông', 203: 'đau', 204: 'bốn', 205: 'bây', 206: 'sân', 207: 'xong', 208: 'sông', 209: 'tri', 210: 'rước', 211: 'kinh', 212: 'bề', 213: 'đất', 214: 'nặng', 215: 'sương', 216: 'nghĩa', 217: 'lầu', 218: 'vương', 219: 'thu', 220: 'cỏ', 221: 'chân', 222: 'bay', 223: 'thơ', 224: 'thuyền', 225: 'mối', 226: 'dài', 227: 'đó', 228: 'nhiều', 229: 'dặm', 230: 'kim', 231: 'đoạn', 232: 'dạy', 233: 'nữa', 234: 'sang', 235: 'đá', 236: 'xin', 237: 'dù', 238: 'quyết', 239: 'cơ', 240: 'vui', 241: 'tai', 242: 'mày', 243: 'già', 244: 'muôn', 245: 'thường', 246: 'tóc', 247: 'màu', 248: 'thiên', 249: 'quanh', 250: 'chiều', 251: 'dường', 252: 'nhìn', 253: 'giọt', 254: 'vâng', 255: 'liệu', 256: 'mụ', 257: 'sư', 258: 'liễu', 259: 'thành', 260: 'đòi', 261: 'hồ', 262: 'lễ', 263: 'thoắt', 264: 'khách', 265: 'cây', 266: 'cờ', 267: 'ngay', 268: 'tính', 269: 'may', 270: 'mừng', 271: 'rõ', 272: 'thúc', 273: 'sắc', 274: 'em', 275: 'thần', 276: 'anh', 277: 'tử', 278: 'ngang', 279: 'gọi', 280: 'tâm', 281: 'ngồi', 282: 'thêm', 283: 'tên', 284: 'giá', 285: 'chốn', 286: 'kể', 287: 'quá', 288: 'quê', 289: 'việc', 290: 'sớm', 291: 'vân', 292: 'vẻ', 293: 'xe', 294: 'cầu', 295: 'sa', 296: 'rành', 297: 'tương', 298: 'lá', 299: 'gương', 300: 'bời', 301: 'lạc', 302: 'ý', 303: 'canh', 304: 'tấm', 305: 'lửa', 306: 'mở', 307: 'bán', 308: 'tàn', 309: 'thiếp', 310: 'đèn', 311: 'cung', 312: 'ăn', 313: 'áo', 314: 'bình', 315: 'giữa', 316: 'đứng', 317: 'e', 318: 'tỉnh', 319: 'nguyệt', 320: 'đền', 321: 'bắt', 322: 'giữ', 323: 'thề', 324: 'rời', 325: 'bằng', 326: 'đánh', 327: 'minh', 328: 'tinh', 329: 'nghề', 330: 'đông', 331: 'vài', 332: 'nổi', 333: 'danh', 334: 'khôn', 335: 'cơn', 336: 'gieo', 337: 'khen', 338: 'thừa', 339: 'cánh', 340: 'rụng', 341: 'ngơ', 342: 'hôm', 343: 'ràng', 344: 'sẽ', 345: 'phụ', 346: 'phen', 347: 'lâm', 348: 'dong', 349: 'hồn', 350: 'giác', 351: 'khéo', 352: 'quen', 353: 'họ', 354: 'khúc', 355: 'trướng', 356: 'mặc', 357: 'ngựa', 358: 'buồng', 359: 'mái', 360: 'bèo', 361: 'sóng', 362: 'bỗng', 363: 'động', 364: 'của', 365: 'rày', 366: 'nhờ', 367: 'lắm', 368: 'ruột', 369: 'hàng', 370: 'lạy', 371: 'gan', 372: 'bất', 373: 'hùng', 374: 'mệnh', 375: 'giở', 376: 'chị', 377: 'đủ', 378: 'dần', 379: 'thác', 380: 'chồng', 381: 'câu', 382: 'dứt', 383: 'buồn', 384: 'âu', 385: 'chầy', 386: 'trúc', 387: 'kỳ', 388: 'ngẩn', 389: 'hợp', 390: 'chiếc', 391: 'tại', 392: 'đổi', 393: 'ngậm', 394: 'chia', 395: 'tiếc', 396: 'mọi', 397: 'nhiêu', 398: 'trả', 399: 'say', 400: 'đình', 401: 'sâu', 402: 'binh', 403: 'phương', 404: 'cách', 405: 'trọng', 406: 'họa', 407: 'rủ', 408: 'tây', 409: 'than', 410: 'vô', 411: 'mê', 412: 'mắt', 413: 'tạ', 414: 'tỏ', 415: 'thầm', 416: 'la', 417: 'lúc', 418: 'khuya', 419: 'nợ', 420: 'cha', 421: 'phúc', 422: 'dày', 423: 'thật', 424: 'thẹn', 425: 'dẫu', 426: 'buộc', 427: 'sai', 428: 'hiếu', 429: 'đùng', 430: 'cũ', 431: 'nối', 432: 'dòng', 433: 'vẹn', 434: 'nét', 435: 'hơn', 436: 'vốn', 437: 'vắng', 438: 'nhan', 439: 'khóc', 440: 'trận', 441: 'lân', 442: 'giải', 443: 'tiện', 444: 'gót', 445: 'bỏ', 446: 'huyên', 447: 'buổi', 448: 'cứ', 449: 'tấc', 450: 'hẳn', 451: 'nọ', 452: 'đài', 453: 'mang', 454: 'sợ', 455: 'nồng', 456: 'quản', 457: 'nát', 458: 'cân', 459: 'nghi', 460: 'máu', 461: 'thong', 462: 'chịu', 463: 'tra', 464: 'tư', 465: 'thúy', 466: 'trang', 467: 'vời', 468: 'âm', 469: 'lưu', 470: 'hội', 471: 'cảnh', 472: 'xao', 473: 'chừng', 474: 'sống', 475: 'tần', 476: 'tưởng', 477: 'chờ', 478: 'hồi', 479: 'yêu', 480: 'cả', 481: 'giục', 482: 'trở', 483: 'lối', 484: 'liền', 485: 'thấp', 486: 'cạn', 487: 'phai', 488: 'cam', 489: 'yên', 490: 'giấc', 491: 'sở', 492: 'đạo', 493: 'xuôi', 494: 'gửi', 495: 'cất', 496: 'uy', 497: 'cõi', 498: 'má', 499: 'truyền', 500: 'trung', 501: 'tuyết', 502: 'phân', 503: 'mặn', 504: 'trắng', 505: 'sắm', 506: 'tà', 507: 'rơi', 508: 'tờ', 509: 'xiết', 510: 'tích', 511: 'lầm', 512: 'ngần', 513: 'đổ', 514: 'nẻo', 515: 'dao', 516: 'nguyên', 517: 'hạ', 518: 'tú', 519: 'trôi', 520: 'bụi', 521: 'hơi', 522: 'thâm', 523: 'chim', 524: 'dạo', 525: 'thiệt', 526: 'tức', 527: 'bày', 528: 'vườn', 529: 'mã', 530: 'nỡ', 531: 'dặn', 532: 'lỡ', 533: 'bẻ', 534: 'giấu', 535: 'phòng', 536: 'ầm', 537: 'tội', 538: 'vành', 539: 'phật', 540: 'thói', 541: 'ghen', 542: 'viên', 543: 'lựa', 544: 'tuần', 545: 'ong', 546: 'tận', 547: 'tháng', 548: 'yến', 549: 'sửa', 550: 'nao', 551: 'nhỏ', 552: 'cuối', 553: 'nhi', 554: 'mong', 555: 'gỡ', 556: 'vần', 557: 'cái', 558: 'thực', 559: 'văn', 560: 'băng', 561: 'nết', 562: 'tha', 563: 'sen', 564: 'mượn', 565: 'bức', 566: 'khuyên', 567: 'xăm', 568: 'cúi', 569: 'xương', 570: 'chỉ', 571: 'lẽ', 572: 'trao', 573: 'tuồng', 574: 'tu', 575: 'dở', 576: 'nhặt', 577: 'ngờ', 578: 'luống', 579: 'sắt', 580: 'gối', 581: 'mất', 582: 'phu', 583: 'dương', 584: 'hoàng', 585: 'quyên', 586: 'tuyền', 587: 'thầy', 588: 'đen', 589: 'mắc', 590: 'dạ', 591: 'ái', 592: 'rỉ', 593: 'buôn', 594: 'truy', 595: 'hình', 596: 'cát', 597: 'rừng', 598: 'báo', 599: 'miệng', 600: 'lệnh', 601: 'gươm', 602: 'rộng', 603: 'mời', 604: 'dâu', 605: 'triều', 606: 'lặng', 607: 'ngoại', 608: 'khác', 609: 'khuôn', 610: 'nở', 611: 'thắm', 612: 'thông', 613: 'êm', 614: 'bướm', 615: 'hành', 616: 'kéo', 617: 'dàu', 618: 'xôn', 619: 'mòn', 620: 'tự', 621: 'niềm', 622: 'khí', 623: 'bồi', 624: 'dùng', 625: 'đề', 626: 'lệ', 627: 'lồng', 628: 'hải', 629: 'mành', 630: 'tựa', 631: 'dò', 632: 'hàn', 633: 'bút', 634: 'ví', 635: 'treo', 636: 'chiêm', 637: 'chợt', 638: 'ngây', 639: 'mơ', 640: 'dời', 641: 'ngẫm', 642: 'sá', 643: 'tròn', 644: 'cậy', 645: 'vòng', 646: 'kết', 647: 'kíp', 648: 'tiệc', 649: 'oán', 650: 'tòng', 651: 'ép', 652: 'giãi', 653: 'trót', 654: 'kêu', 655: 'hại', 656: 'qui', 657: 'vậy', 658: 'đương', 659: 'làng', 660: 'thảm', 661: 'phấn', 662: 'toan', 663: 'khơi', 664: 'vực', 665: 'thị', 666: 'phi', 667: 'rạng', 668: 'khanh', 669: 'chước', 670: 'phục', 671: 'khỏi', 672: 'am', 673: 'cuộc', 674: 'trai', 675: 'ả', 676: 'da', 677: 'thủy', 678: 'tới', 679: 'tiết', 680: 'nức', 681: 'dìu', 682: 'ước', 683: 'vợ', 684: 'ma', 685: 'loan', 686: 'kiếm', 687: 'nằm', 688: 'linh', 689: 'chửa', 690: 'chào', 691: 'quốc', 692: 'cố', 693: 'tứ', 694: 'sụt', 695: 'sùi', 696: 'suy', 697: 'hiên', 698: 'ngộ', 699: 'đóng', 700: 'sàng', 701: 'thoa', 702: 'các', 703: 'trầm', 704: 'đợi', 705: 'xôi', 706: 'khăn', 707: 'mẹ', 708: 'lượng', 709: 'lùng', 710: 'giao', 711: 'tươi', 712: 'nguyền', 713: 'phường', 714: 'tướng', 715: 'tàng', 716: 'ngắn', 717: 'giã', 718: 'rèm', 719: 'án', 720: 'giáp', 721: 'màng', 722: 'ninh', 723: 'chiêu', 724: 'vò', 725: 'trinh', 726: 'gởi', 727: 'vơ', 728: 'gánh', 729: 'vầy', 730: 'giang', 731: 'tạm', 732: 'ly', 733: 'hèn', 734: 'thử', 735: 'rắp', 736: 'mảnh', 737: 'đặt', 738: 'thịt', 739: 'vật', 740: 'bồ', 741: 'ôi', 742: 'kiệu', 743: 'nắng', 744: 'loài', 745: 'hầu', 746: 'ngàn', 747: 'thờ', 748: 'son', 749: 'lìa', 750: 'lập', 751: 'tràng', 752: 'thuận', 753: 'thất', 754: 'thiếu', 755: 'trống', 756: 'thơm', 757: 'lục', 758: 'vững', 759: 'thua', 760: 'pha', 761: 'mùi', 762: 'não', 763: 'quần', 764: 'màn', 765: 'chín', 766: 'mươi', 767: 'điểm', 768: 'dập', 769: 'đàng', 770: 'đạm', 771: 'dấu', 772: 'rêu', 773: 'chủ', 774: 'thăm', 775: 'đầm', 776: 'vùng', 777: 'hiu', 778: 'lau', 779: 'phách', 780: 'ắt', 781: 'kịp', 782: 'cuốn', 783: 'ít', 784: 'in', 785: 'chớ', 786: 'dằng', 787: 'lưng', 788: 'chảy', 789: 'gác', 790: 'trình', 791: 'quả', 792: 'thềm', 793: 'cớ', 794: 'mua', 795: 'dào', 796: 'khóa', 797: 'the', 798: 'phím', 799: 'trà', 800: 'nghiêm', 801: 'suốt', 802: 'việt', 803: 'hằng', 804: 'thấm', 805: 'gấm', 806: 'ơn', 807: 'gạn', 808: 'thang', 809: 'lừa', 810: 'soi', 811: 'khiến', 812: 'ngại', 813: 'đang', 814: 'rẽ', 815: 'sánh', 816: 'thảo', 817: 'định', 818: 'đừng', 819: 'đục', 820: 'đắng', 821: 'cay', 822: 'tiêu', 823: 'xiêu', 824: 'trơ', 825: 'ngừng', 826: 'ngất', 827: 'đoàn', 828: 'nha', 829: 'miền', 830: 'đáng', 831: 'thà', 832: 'lo', 833: 'han', 834: 'thuốc', 835: 'hẹn', 836: 'chọn', 837: 'hôn', 838: 'kề', 839: 'gà', 840: 'vả', 841: 'tủi', 842: 'lụy', 843: 'muốn', 844: 'đằng', 845: 'đăm', 846: 'lớn', 847: 'tam', 848: 'bảo', 849: 'phép', 850: 'chạy', 851: 'thú', 852: 'chuyện', 853: 'trò', 854: 'hiểm', 855: 'nghiệp', 856: 'bàn', 857: 'răng', 858: 'hoàn', 859: 'an', 860: 'phiền', 861: 'pháp', 862: 'thiền', 863: 'hùm', 864: 'đớn', 865: 'nga', 866: 'cốt', 867: 'ngài', 868: 'đoan', 869: 'sơn', 870: 'ngâm', 871: 'bậc', 872: 'đứt', 873: 'chương', 874: 'én', 875: 'thoi', 876: 'quang', 877: 'khói', 878: 'dẫn', 879: 'trâm', 880: 'lạnh', 881: 'thỏ', 882: 'ác', 883: 'to', 884: 'ủ', 885: 'tuôn', 886: 'khó', 887: 'hiển', 888: 'đè', 889: 'từng', 890: 'lai', 891: 'hậu', 892: 'trộm', 893: 'chốc', 894: 'lan', 895: 'chênh', 896: 'lĩnh', 897: 'vẫy', 898: 'dồi', 899: 'dầm', 900: 'số', 901: 'mộng', 902: 'oanh', 903: 'kín', 904: 'bâng', 905: 'khuâng', 906: 'vi', 907: 'then', 908: 'cài', 909: 'ngô', 910: 'thuê', 911: 'dọn', 912: 'lọt', 913: 'vuông', 914: 'trẻ', 915: 'bó', 916: 'đeo', 917: 'ghi', 918: 'quạt', 919: 'quì', 920: 'viện', 921: 'dải', 922: 'dâng', 923: 'thức', 924: 'thai', 925: 'vai', 926: 'nhiên', 927: 'cần', 928: 'chúa', 929: 'hạc', 930: 'mau', 931: 'chau', 932: 'tao', 933: 'bớt', 934: 'ngăn', 935: 'chán', 936: 'chắp', 937: 'bàng', 938: 'ngân', 939: 'bơ', 940: 'tang', 941: 'lẻn', 942: 'tành', 943: 'đồ', 944: 'lòa', 945: 'lọ', 946: 'quyền', 947: 'lao', 948: 'phó', 949: 'đại', 950: 'trạc', 951: 'ưa', 952: 'vu', 953: 'đọa', 954: 'coi', 955: 'địa', 956: 'sức', 957: 'độ', 958: 'lang', 959: 'chìm', 960: 'tối', 961: 'trùng', 962: 'ngôi', 963: 'tầm', 964: 'dơ', 965: 'giận', 966: 'kẻo', 967: 'góc', 968: 'gái', 969: 'dạn', 970: 'thẳng', 971: 'hạnh', 972: 'rượu', 973: 'vơi', 974: 'dại', 975: 'hòa', 976: 'sấm', 977: 'thở', 978: 'ấm', 979: 'hoạn', 980: 'bõ', 981: 'gian', 982: 'vị', 983: 'nạn', 984: 'thổ', 985: 'nữ', 986: 'cá', 987: 'dắt', 988: 'thốt', 989: 'nhường', 990: 'so', 991: 'trương', 992: 'mực', 993: 'che', 994: 'bông', 995: 'nô', 996: 'bộ', 997: 'giai', 998: 'ngổn', 999: 'đống', 1000: 'ngả', 1001: 'ghềnh', 1002: 'ghé', 1003: 'mờ', 1004: 'vùi', 1005: 'mồ', 1006: 'phũ', 1007: 'phàng', 1008: 'phượng', 1009: 'tham', 1010: 'rầm', 1011: 'rút', 1012: 'vạch', 1013: 'vắn', 1014: 'dư', 1015: 'đấng', 1016: 'thể', 1017: 'ào', 1018: 'giày', 1019: 'đều', 1020: 'hữu', 1021: 'gốc', 1022: 'buông', 1023: 'khấu', 1024: 'túi', 1025: 'thằng', 1026: 'hài', 1027: 'vẫn', 1028: 'thêu', 1029: 'phùng', 1030: 'cúc', 1031: 'lả', 1032: 'nương', 1033: 'thiu', 1034: 'đón', 1035: 'ban', 1036: 'quên', 1037: 'mé', 1038: 'vẽ', 1039: 'khuây', 1040: 'hao', 1041: 'gây', 1042: 'ngươi', 1043: 'mọc', 1044: 'lam', 1045: 'học', 1046: 'sách', 1047: 'khép', 1048: 'mù', 1049: 'khinh', 1050: 'lý', 1051: 'dừng', 1052: 'ấp', 1053: 'xét', 1054: 'ru', 1055: 'nể', 1056: 'tạc', 1057: 'cởi', 1058: 'năng', 1059: 'xiêm', 1060: 'dành', 1061: 'đắp', 1062: 'rào', 1063: 'dung', 1064: 'mỏng', 1065: 'đoán', 1066: 'phát', 1067: 'vầng', 1068: 'đinh', 1069: 'hà', 1070: 'chiến', 1071: 'chen', 1072: 'lứa', 1073: 'giàng', 1074: 'bởi', 1075: 'nài', 1076: 'ngỏ', 1077: 'rối', 1078: 'ôm', 1079: 'nóc', 1080: 'thước', 1081: 'đao', 1082: 'sanh', 1083: 'dưng', 1084: 'dậy', 1085: 'ngược', 1086: 'khốc', 1087: 'biến', 1088: 'đức', 1089: 'vay', 1090: 'giám', 1091: 'huyện', 1092: 'niên', 1093: 'gầy', 1094: 'cực', 1095: 'khuất', 1096: 'can', 1097: 'cỗi', 1098: 'khăng', 1099: 'dang', 1100: 'lỗi', 1101: 'tái', 1102: 'cải', 1103: 'khắc', 1104: 'nam', 1105: 'tằm', 1106: 'đứa', 1107: 'rủi', 1108: 'quý', 1109: 'bén', 1110: 'hoặc', 1111: 'đuốc', 1112: 'giống', 1113: 'tiễn', 1114: 'ruổi', 1115: 'trì', 1116: 'thẳm', 1117: 'san', 1118: 'sát', 1119: 'biếc', 1120: 'xấu', 1121: 'chiếu', 1122: 'giường', 1123: 'vỡ', 1124: 'chật', 1125: 'thâu', 1126: 'chực', 1127: 'bát', 1128: 'buồm', 1129: 'nội', 1130: 'ngùi', 1131: 'dàng', 1132: 'nhắn', 1133: 'vớt', 1134: 'lấp', 1135: 'đơn', 1136: 'hãi', 1137: 'bạch', 1138: 'phù', 1139: 'lương', 1140: 'mãi', 1141: 'thuộc', 1142: 'mài', 1143: 'trưa', 1144: 'hàm', 1145: 'thoát', 1146: 'sét', 1147: 'phủ', 1148: 'dẹp', 1149: 'lường', 1150: 'bưng', 1151: 'bào', 1152: 'kiến', 1153: 'mưu', 1154: 'khuyển', 1155: 'ưng', 1156: 'quỷ', 1157: 'chắc', 1158: 'trí', 1159: 'dãi', 1160: 'bồng', 1161: 'thù', 1162: 'rồng', 1163: 'nhẹ', 1164: 'lánh', 1165: 'lành', 1166: 'dưa', 1167: 'thênh', 1168: 'gồm', 1169: 'mênh', 1170: 'rỡ', 1171: 'vinh', 1172: 'lăm', 1173: 'ghét', 1174: 'trải', 1175: 'thứ', 1176: 'hờn', 1177: 'nghiêng', 1178: 'thi', 1179: 'ca', 1180: 'ngũ', 1181: 'lê', 1182: 'đạp', 1183: 'vó', 1184: 'tro', 1185: 'dan', 1186: 'bắc', 1187: 'tanh', 1188: 'ngắt', 1189: 'trâu', 1190: 'phôi', 1191: 'khắp', 1192: 'chạ', 1193: 'hoài', 1194: 'nén', 1195: 'suối', 1196: 'khấn', 1197: 'gật', 1198: 'thuở', 1199: 'chừa', 1200: 'vận', 1201: 'nề', 1202: 'dậm', 1203: 'lộc', 1204: 'láng', 1205: 'cổ', 1206: 'huề', 1207: 'quỳnh', 1208: 'nép', 1209: 'nền', 1210: 'hào', 1211: 'cấu', 1212: 'nhác', 1213: 'chập', 1214: 'chỉn', 1215: 'ngắm', 1216: 'tuyệt', 1217: 'xế', 1218: 'sổ', 1219: 'tập', 1220: 'nhất', 1221: 'giọng', 1222: 'dề', 1223: 'chuốc', 1224: 'mạch', 1225: 'đong', 1226: 'khuyết', 1227: 'đĩa', 1228: 'trêu', 1229: 'du', 1230: 'gang', 1231: 'nguồn', 1232: 'nhẫn', 1233: 'khuê', 1234: 'thoảng', 1235: 'hư', 1236: 'phỉ', 1237: 'gắn', 1238: 'hẹp', 1239: 'đuổi', 1240: 'ngùng', 1241: 'đa', 1242: 'tuổi', 1243: 'chở', 1244: 'nhật', 1245: 'thời', 1246: 'trân', 1247: 'trách', 1248: 'hờ', 1249: 'hững', 1250: 'vạn', 1251: 'phẩm', 1252: 'khoảng', 1253: 'nhắc', 1254: 'môn', 1255: 'đoài', 1256: 'hòe', 1257: 'căn', 1258: 'vặn', 1259: 'lộn', 1260: 'mát', 1261: 'chày', 1262: 'xích', 1263: 'khang', 1264: 'quảng', 1265: 'khoan', 1266: 'nuốt', 1267: 'tẻ', 1268: 'yếm', 1269: 'lơi', 1270: 'đóa', 1271: 'đẹp', 1272: 'tày', 1273: 'lạt', 1274: 'liêu', 1275: 'mảng', 1276: 'ngập', 1277: 'họp', 1278: 'nhạn', 1279: 'vấn', 1280: 'sôi', 1281: 'lão', 1282: 'gói', 1283: 'sạch', 1284: 'giật', 1285: 'xuất', 1286: 'nhục', 1287: 'chuộc', 1288: 'luồn', 1289: 'lạng', 1290: 'tớ', 1291: 'điệu', 1292: 'tùy', 1293: 'dặt', 1294: 'rìu', 1295: 'nghì', 1296: 'hỡi', 1297: 'bia', 1298: 'huyền', 1299: 'trú', 1300: 'nghênh', 1301: 'gáy', 1302: 'hổ', 1303: 'chôn', 1304: 'nghỉ', 1305: 'gạt', 1306: 'cô', 1307: 'nguyện', 1308: 'bì', 1309: 'cắt', 1310: 'trốn', 1311: 'xứng', 1312: 'rạch', 1313: 'túc', 1314: 'đãi', 1315: 'nguôi', 1316: 'rửa', 1317: 'đày', 1318: 'luân', 1319: 'thót', 1320: 'lay', 1321: 'bọt', 1322: 'tơi', 1323: 'gùng', 1324: 'nhé', 1325: 'giếng', 1326: 'sáng', 1327: 'bảy', 1328: 'tác', 1329: 'gượng', 1330: 'hoan', 1331: 'tòa', 1332: 'đội', 1333: 'quế', 1334: 'co', 1335: 'điếm', 1336: 'ngõ', 1337: 'đấy', 1338: 'nỉ', 1339: 'phút', 1340: 'đòn', 1341: 'lũy', 1342: 'tăm', 1343: 'đáy', 1344: 'nhủ', 1345: 'đèo', 1346: 'sòng', 1347: 'dọc', 1348: 'thảnh', 1349: 'thơi', 1350: 'tẩy', 1351: 'mùa', 1352: 'ê', 1353: 'côn', 1354: 'vượt', 1355: 'bến', 1356: 'ngọt', 1357: 'lũ', 1358: 'tưới', 1359: 'quàng', 1360: 'di', 1361: 'vã', 1362: 'ngành', 1363: 'tì', 1364: 'y', 1365: 'vách', 1366: 'chan', 1367: 'trái', 1368: 'thánh', 1369: 'thiết', 1370: 'diện', 1371: 'bi', 1372: 'chép', 1373: 'nâu', 1374: 'sồng', 1375: 'bách', 1376: 'viết', 1377: 'khổ', 1378: 'gai', 1379: 'lênh', 1380: 'đênh', 1381: 'trưởng', 1382: 'hiến', 1383: 'muối', 1384: 'khêu', 1385: 'thích', 1386: 'biên', 1387: 'kỷ', 1388: 'mồi', 1389: 'dựng', 1390: 'khao', 1391: 'tranh', 1392: 'vỗ', 1393: 'thiêng', 1394: 'mông', 1395: 'cứu', 1396: 'tuy', 1397: 'chăn', 1398: 'nhậm', 1399: 'bỉ', 1400: 'cảo', 1401: 'tĩnh', 1402: 'nho', 1403: 'tố', 1404: 'nang', 1405: 'sảo', 1406: 'làn', 1407: 'kém', 1408: 'rất', 1409: 'cập', 1410: 'kê', 1411: 'chục', 1412: 'sáu', 1413: 'mộ', 1414: 'nêm', 1415: 'thẩn', 1416: 'khê', 1417: 'uốn', 1418: 'dịp', 1419: 'sè', 1420: 'nấm', 1421: 'manh', 1422: 'viễn', 1423: 'gẫy', 1424: 'lờ', 1425: 'nông', 1426: 'đoái', 1427: 'giắt', 1428: 'vịnh', 1429: 'mẩn', 1430: 'dột', 1431: 'nực', 1432: 'rung', 1433: 'nấy', 1434: 'nhạc', 1435: 'phú', 1436: 'tót', 1437: 'đố', 1438: 'rốn', 1439: 'thướt', 1440: 'núi', 1441: 'chiêng', 1442: 'dòm', 1443: 'rộn', 1444: 'ngụ', 1445: 'tân', 1446: 'lãng', 1447: 'đãng', 1448: 'dạt', 1449: 'lớp', 1450: 'dưỡng', 1451: 'mả', 1452: 'triệu', 1453: 'nách', 1454: 'biếng', 1455: 'ghê', 1456: 'ngao', 1457: 'ngán', 1458: 'se', 1459: 'phất', 1460: 'khuynh', 1461: 'vắt', 1462: 'lô', 1463: 'hắt', 1464: 'lơ', 1465: 'quán', 1466: 'sượng', 1467: 'sùng', 1468: 'rũ', 1469: 'si', 1470: 'thòi', 1471: 'thía', 1472: 'rậm', 1473: 'biện', 1474: 'nhuốm', 1475: 'giả', 1476: 'tùng', 1477: 'táp', 1478: 'liếc', 1479: 'bội', 1480: 'sĩ', 1481: 'dầy', 1482: 'phơi', 1483: 'phới', 1484: 'đỉnh', 1485: 'sáp', 1486: 'nện', 1487: 'kỹ', 1488: 'bận', 1489: 'dẽ', 1490: 'bố', 1491: 'chường', 1492: 'rẻ', 1493: 'bẽ', 1494: 'chính', 1495: 'hộ', 1496: 'thớt', 1497: 'gìn', 1498: 'sum', 1499: 'quảy', 1500: 'xẻ', 1501: 'trác', 1502: 'xác', 1503: 'vang', 1504: 'dệt', 1505: 'tế', 1506: 'xưng', 1507: 'tuất', 1508: 'nhưng', 1509: 'cù', 1510: 'thệ', 1511: 'lót', 1512: 'biệt', 1513: 'hạt', 1514: 'đồn', 1515: 'râu', 1516: 'ghế', 1517: 'sỗ', 1518: 'vén', 1519: 'đắn', 1520: 'đo', 1521: 'sính', 1522: 'gấp', 1523: 'ngã', 1524: 'nạp', 1525: 'thái', 1526: 'búa', 1527: 'vôi', 1528: 'rã', 1529: 'ký', 1530: 'giúp', 1531: 'tụng', 1532: 'khít', 1533: 'ráo', 1534: 'chong', 1535: 'thổn', 1536: 'vướng', 1537: 'hở', 1538: 'thùng', 1539: 'đốt', 1540: 'lò', 1541: 'vàn', 1542: 'rầu', 1543: 'nhị', 1544: 'long', 1545: 'gã', 1546: 'ngoa', 1547: 'tôn', 1548: 'dến', 1549: 'lựu', 1550: 'lận', 1551: 'quỳ', 1552: 'xá', 1553: 'mi', 1554: 'tã', 1555: 'hôi', 1556: 'khấp', 1557: 'lã', 1558: 'chã', 1559: 'bợm', 1560: 'dắng', 1561: 'nằn', 1562: 'nì', 1563: 'yếu', 1564: 'tạnh', 1565: 'nhợt', 1566: 'hoi', 1567: 'rập', 1568: 'vẩn', 1569: 'cướp', 1570: 'tốt', 1571: 'ngứa', 1572: 'sấn', 1573: 'nghiệt', 1574: 'mơn', 1575: 'man', 1576: 'ròi', 1577: 'dối', 1578: 'ngát', 1579: 'thoáng', 1580: 'chuốt', 1581: 'dịu', 1582: 'đai', 1583: 'tháo', 1584: 'vọng', 1585: 'chua', 1586: 'nhạt', 1587: 'lữa', 1588: 'đáp', 1589: 'nhắm', 1590: 'xoay', 1591: 'trút', 1592: 'tía', 1593: 'hầm', 1594: 'áp', 1595: 'hung', 1596: 'nàn', 1597: 'quyến', 1598: 'mắng', 1599: 'chê', 1600: 'ngợi', 1601: 'cợt', 1602: 'đỡ', 1603: 'thấu', 1604: 'võ', 1605: 'tô', 1606: 'nùng', 1607: 'mận', 1608: 'bầu', 1609: 'vây', 1610: 'đúc', 1611: 'nấn', 1612: 'ná', 1613: 'lôi', 1614: 'chàm', 1615: 'cáo', 1616: 'phiếu', 1617: 'luận', 1618: 'mẫu', 1619: 'quẹn', 1620: 'vóc', 1621: 'đuôi', 1622: 'mộc', 1623: 'dữ', 1624: 'hòng', 1625: 'bòng', 1626: 'ranh', 1627: 'bò', 1628: 'ván', 1629: 'mách', 1630: 'mảy', 1631: 'thau', 1632: 'dông', 1633: 'bụng', 1634: 'chạnh', 1635: 'quy', 1636: 'ngảnh', 1637: 'chề', 1638: 'mầu', 1639: 'lèo', 1640: 'bầy', 1641: 'tề', 1642: 'tuốt', 1643: 'ẩn', 1644: 'hớt', 1645: 'cháy', 1646: 'khâm', 1647: 'đảo', 1648: 'cửu', 1649: 'chiền', 1650: 'xiên', 1651: 'ngục', 1652: 'sảnh', 1653: 'thiêm', 1654: 'a', 1655: 'thắp', 1656: 'thiện', 1657: 'túng', 1658: 'ran', 1659: 'chì', 1660: 'lược', 1661: 'năn', 1662: 'trọn', 1663: 'uyên', 1664: 'nham', 1665: 'giết', 1666: 'lảng', 1667: 'thét', 1668: 'bản', 1669: 'héo', 1670: 'doanh', 1671: 'chạm', 1672: 'thoạt', 1673: 'thụ', 1674: 'chùa', 1675: 'thủ', 1676: 'tắt', 1677: 'lưới', 1678: 'nghiến', 1679: 'lể', 1680: 'nhận', 1681: 'huynh', 1682: 'chuông', 1683: 'khánh', 1684: 'rà', 1685: 'hề', 1686: 'sói', 1687: 'quét', 1688: 'đỗ', 1689: 'khốn', 1690: 'thao', 1691: 'phỏng', 1692: 'nhàn', 1693: 'rợp', 1694: 'rứt', 1695: 'ngó', 1696: 'chầu', 1697: 'súng', 1698: 'nhàng', 1699: 'chấp', 1700: 'vệ', 1701: 'chỉnh', 1702: 'bác', 1703: 'phạm', 1704: 'tù', 1705: 'kính', 1706: 'ngoan', 1707: 'thùy', 1708: 'dự', 1709: 'đắc', 1710: 'ngác', 1711: 'nhơn', 1712: 'chuyển', 1713: 'loạn', 1714: 'dũng', 1715: 'ngón', 1716: 've', 1717: 'phổ', 1718: 'đắm', 1719: 'khư', 1720: 'lâng', 1721: 'ngư', 1722: 'duềnh', 1723: 'mướn', 1724: 'nó', 1725: 'dặc', 1726: 'thẫn', 1727: 'khát', 1728: 'đô', 1729: 'thục', 1730: 'sử', 1731: 'phẳng', 1732: 'bực', 1733: 'rốt', 1734: 'đặn', 1735: 'làu', 1736: 'sấp', 1737: 'xỉ', 1738: 'đềm', 1739: 'thiều', 1740: 'tảo', 1741: 'gò', 1742: 'rắc', 1743: 'giấy', 1744: 'hiếm', 1745: 'gãy', 1746: 'nếp', 1747: 'lặn', 1748: 'viếng', 1749: 'hoá', 1750: 'mỏi', 1751: 'khứa', 1752: 'sụp', 1753: 'áy', 1754: 'thổi', 1755: 'u', 1756: 'hiện', 1757: 'lỏng', 1758: 'giòn', 1759: 'nhuộm', 1760: 'quất', 1761: 'mạo', 1762: 'nhã', 1763: 'tước', 1764: 'khoá', 1765: 'mòng', 1766: 'thoả', 1767: 'chờn', 1768: 'nghé', 1769: 'veo', 1770: 'chếch', 1771: 'ngấn', 1772: 'phồn', 1773: 'diệu', 1774: 'triện', 1775: 'ném', 1776: 'khẩu', 1777: 'cẩm', 1778: 'xịch', 1779: 'lưỡng', 1780: 'lự', 1781: 'rền', 1782: 'rĩ', 1783: 'trằn', 1784: 'trọc', 1785: 'nhắp', 1786: 'ứng', 1787: 'thẻ', 1788: 'trạnh', 1789: 'lắc', 1790: 'dồn', 1791: 'lẽo', 1792: 'đẽo', 1793: 'chùng', 1794: 'khan', 1795: 'rì', 1796: 'gợi', 1797: 'khảy', 1798: 'cổng', 1799: 'mỉa', 1800: 'cặp', 1801: 'lãm', 1802: 'tịt', 1803: 'thèm', 1804: 'xốc', 1805: 'giơ', 1806: 'báu', 1807: 'nghía', 1808: 'thoang', 1809: 'tòi', 1810: 'ướm', 1811: 'phố', 1812: 'thiểu', 1813: 'khuyến', 1814: 'rụt', 1815: 'rè', 1816: 'ngẫu', 1817: 'nhỉ', 1818: 'chồn', 1819: 'ngừ', 1820: 'chất', 1821: 'ích', 1822: 'hòi', 1823: 'bả', 1824: 'tất', 1825: 'sờ', 1826: 'tưng', 1827: 'bừng', 1828: 'thoăn', 1829: 'dặng', 1830: 'râm', 1831: 'tệ', 1832: 'xắn', 1833: 'góp', 1834: 'phác', 1835: 'và', 1836: 'nhả', 1837: 'phun', 1838: 'chuồn', 1839: 'thắng', 1840: 'dai', 1841: 'giọi', 1842: 'huỳnh', 1843: 'món', 1844: 'vằng', 1845: 'vặc', 1846: 'khân', 1847: 'sàm', 1848: 'sỡ', 1849: 'diệp', 1850: 'lắng', 1851: 'nâng', 1852: 'mọn', 1853: 'vũ', 1854: 'hán', 1855: 'lăng', 1856: 'luyến', 1857: 'sầm', 1858: 'sập', 1859: 'đầụ', 1860: 'bộc', 1861: 'xổi', 1862: 'rúng', 1863: 'đằm', 1864: 'tàu', 1865: 'sài', 1866: 'lữ', 1867: 'thấn', 1868: 'cữ', 1869: 'vít', 1870: 'xơ', 1871: 'thọ', 1872: 'ruồi', 1873: 'khung', 1874: 'nhuyễn', 1875: 'sành', 1876: 'vét', 1877: 'đan', 1878: 'giàm', 1879: 'hoảng', 1880: 'hốt', 1881: 'van', 1882: 'điếc', 1883: 'tồi', 1884: 'rường', 1885: 'dịch', 1886: 'giam', 1887: 'vạ', 1888: 'nhẵn', 1889: 'nhụi', 1890: 'bảnh', 1891: 'giợn', 1892: 'cò', 1893: 'kè', 1894: 'dằm', 1895: 'khất', 1896: 'nuôi', 1897: 'vác', 1898: 'nưóc', 1899: 'đậu', 1900: 'quẩn', 1901: 'độc', 1902: 'đẳng', 1903: 'khối', 1904: 'tràn', 1905: 'môi', 1906: 'mủ', 1907: 'lây', 1908: 'rẩy', 1909: 'ngủi', 1910: 'chẩy', 1911: 'giất', 1912: 'tấp', 1913: 'nập', 1914: 'dằn', 1915: 'vựng', 1916: 'lậy', 1917: 'chiềng', 1918: 'chéo', 1919: 'mạt', 1920: 'cưa', 1921: 'mướp', 1922: 'lề', 1923: 'chợ', 1924: 'lái', 1925: 'mẹo', 1926: 'đua', 1927: 'miếng', 1928: 'ngon', 1929: 'phàm', 1930: 'vin', 1931: 'quít', 1932: 'vỏ', 1933: 'mào', 1934: 'mập', 1935: 'căm', 1936: 'ô', 1937: 'giãn', 1938: 'rúc', 1939: 'còi', 1940: 'khểnh', 1941: 'gập', 1942: 'rớp', 1943: 'giấn', 1944: 'thui', 1945: 'thủi', 1946: 'nhiệm', 1947: 'kìa', 1948: 'quỉ', 1949: 'phau', 1950: 'xen', 1951: 'bánh', 1952: 'nhờn', 1953: 'đẫy', 1954: 'tượng', 1955: 'lông', 1956: 'phụng', 1957: 'vía', 1958: 'lột', 1959: 'hỏa', 1960: 'cậu', 1961: 'bành', 1962: 'min', 1963: 'mần', 1964: 'liếng', 1965: 'nhập', 1966: 'văng', 1967: 'lở', 1968: 'bằn', 1969: 'bặt', 1970: 'sóc', 1971: 'hò', 1972: 'nhụy', 1973: 'chưn', 1974: 'mỗi', 1975: 'ngưng', 1976: 'bích', 1977: 'cồn', 1978: 'gột', 1979: 'mác', 1980: 'ngồị', 1981: 'chải', 1982: 'cũi', 1983: 'đanh', 1984: 'cám', 1985: 'chơ', 1986: 'mốt', 1987: 'nần', 1988: 'lẩm', 1989: 'nhẩm', 1990: 'kiện', 1991: 'kép', 1992: 'đỗi', 1993: 'tạo', 1994: 'lậu', 1995: 'cương', 1996: 'hóa', 1997: 'nau', 1998: 'vuốt', 1999: 'tốc', 2000: 'hăng', 2001: 'giập', 2002: 'khẩn', 2003: 'chết', 2004: 'lươn', 2005: 'lấm', 2006: 'mo', 2007: 'rao', 2008: 'phao', 2009: 'aỉ', 2010: 'quắt', 2011: 'giẩy', 2012: 'chứ', 2013: 'lui', 2014: 'vụng', 2015: 'bù', 2016: 'tám', 2017: 'lăn', 2018: 'lóc', 2019: 'khóe', 2020: 'khắt', 2021: 'khe', 2022: 'tống', 2023: 'ơ', 2024: 'giùi', 2025: 'chuyên', 2026: 'sỉ', 2027: 'nòi', 2028: 'khôi', 2029: 'mởn', 2030: 'giằng', 2031: 'mắn', 2032: 'miệt', 2033: 'díu', 2034: 'bốc', 2035: 'hễ', 2036: 'hè', 2037: 'loè', 2038: 'đâm', 2039: 'tẩm', 2040: 'ngà', 2041: 'luật', 2042: 'điêu', 2043: 'áng', 2044: 'đột', 2045: 'tênh', 2046: 'lượn', 2047: 'lạch', 2048: 'ngửa', 2049: 'trươc', 2050: 'giấm', 2051: 'dáng', 2052: 'saỏ', 2053: 'lào', 2054: 'sắp', 2055: 'thợ', 2056: 'bắn', 2057: 'đượm', 2058: 'giậu', 2059: 'nảy', 2060: 'giò', 2061: 'huấn', 2062: 'đẫu', 2063: 'nhúng', 2064: 'sốt', 2065: 'sì', 2066: 'thải', 2067: 'trạng', 2068: 'nhện', 2069: 'hoen', 2070: 'hứng', 2071: 'nghiên', 2072: 'phê', 2073: 'thịnh', 2074: 'cưu', 2075: 'nhịp', 2076: 'xúy', 2077: 'huệ', 2078: 'sực', 2079: 'nẩy', 2080: 'tào', 2081: 'ròng', 2082: 'nàọ', 2083: 'mức', 2084: 'loi', 2085: 'bờ', 2086: 'nghẹn', 2087: 'trôn', 2088: 'bữa', 2089: 'chinh', 2090: 'dôi', 2091: 'bít', 2092: 'nhãn', 2093: 'tâng', 2094: 'gớm', 2095: 'mít', 2096: 'khảo', 2097: 'thủng', 2098: 'thỉnh', 2099: 'thuần', 2100: 'hức', 2101: 'lanh', 2102: 'xây', 2103: 'roi', 2104: 'gióng', 2105: 'ghẻ', 2106: 'thinh', 2107: 'mệt', 2108: 'bùi', 2109: 'chấm', 2110: 'sắn', 2111: 'bìm', 2112: 'cỏn', 2113: 'rưới', 2114: 'thây', 2115: 'càn', 2116: 'hơ', 2117: 'na', 2118: 'liệm', 2119: 'vĩnh', 2120: 'giây', 2121: 'đắt', 2122: 'suyền', 2123: 'dỡ', 2124: 'đò', 2125: 'nộp', 2126: 'ngước', 2127: 'dãy', 2128: 'trủng', 2129: 'tể', 2130: 'mèo', 2131: 'lúng', 2132: 'khủng', 2133: 'khỉnh', 2134: 'đập', 2135: 'phiên', 2136: 'chè', 2137: 'liểu', 2138: 'náu', 2139: 'lãnh', 2140: 'giề', 2141: 'quáng', 2142: 'thơn', 2143: 'siêu', 2144: 'ơi', 2145: 'trắc', 2146: 'dĩ', 2147: 'giạm', 2148: 'hòn', 2149: 'tán', 2150: 'hoán', 2151: 'tê', 2152: 'nụ', 2153: 'gảy', 2154: 'bã', 2155: 'khởi', 2156: 'ngầm', 2157: 'bấc', 2158: 'ỉ', 2159: 'rát', 2160: 'giàu', 2161: 'cúng', 2162: 'giới', 2163: 'cà', 2164: 'pho', 2165: 'nồi', 2166: 'đẫm', 2167: 'tông', 2168: 'quằn', 2169: 'quại', 2170: 'vũng', 2171: 'lầy', 2172: 'giông', 2173: 'bâu', 2174: 'ngào', 2175: 'rón', 2176: 'kẽ', 2177: 'sởn', 2178: 'im', 2179: 'nọc', 2180: 'rắn', 2181: 'no', 2182: 'mịt', 2183: 'đồi', 2184: 'gõ', 2185: 'trụ', 2186: 'kệ', 2187: 'bối', 2188: 'phướn', 2189: 'tuệ', 2190: 'tổ', 2191: 'nhớn', 2192: 'xua', 2193: 'phá', 2194: 'chứa', 2195: 'cháu', 2196: 'sẩy', 2197: 'cong', 2198: 'chứng', 2199: 'nhang', 2200: 'nường', 2201: 'chém', 2202: 'phèn', 2203: 'bùn', 2204: 'chèo', 2205: 'vờ', 2206: 'chậu', 2207: 'kén', 2208: 'tấn', 2209: 'cưỡi', 2210: 'trượng', 2211: 'rong', 2212: 'đẵng', 2213: 'giầy', 2214: 'bổng', 2215: 'kình', 2216: 'ngạc', 2217: 'nguy', 2218: 'loa', 2219: 'liễn', 2220: 'phấp', 2221: 'hoả', 2222: 'lộ', 2223: 'rầy', 2224: 'thưởng', 2225: 'om', 2226: 'thòm', 2227: 'rình', 2228: 'nời', 2229: 'tuyển', 2230: 'nã', 2231: 'vãi', 2232: 'mẻ', 2233: 'tóm', 2234: 'xử', 2235: 'giẽ', 2236: 'run', 2237: 'sâm', 2238: 'há', 2239: 'quái', 2240: 'cắp', 2241: 'ướt', 2242: 'xẩy', 2243: 'khiếp', 2244: 'dốn', 2245: 'chư', 2246: 'nắp', 2247: 'tuông', 2248: 'chông', 2249: 'nhen', 2250: 'lộng', 2251: 'tải', 2252: 'cước', 2253: 'nhằm', 2254: 'cạnh', 2255: 'óc', 2256: 'huống', 2257: 'chẻ', 2258: 'ngói', 2259: 'lưỡi', 2260: 'cơm', 2261: 'bá', 2262: 'cường', 2263: 'tổng', 2264: 'đốc', 2265: 'đẩy', 2266: 'đặc', 2267: 'đổng', 2268: 'nhung', 2269: 'thuyết', 2270: 'tung', 2271: 'hoành', 2272: 'láo', 2273: 'chọc', 2274: 'khuấy', 2275: 'truân', 2276: 'dà', 2277: 'dè', 2278: 'trạch', 2279: 'sào', 2280: 'tiếp', 2281: 'sứ', 2282: 'trễ', 2283: 'kế', 2284: 'dàn', 2285: 'ám', 2286: 'hiệu', 2287: 'hoang', 2288: 'dội', 2289: 'triền', 2290: 'toán', 2291: 'miếu', 2292: 'ngỡ', 2293: 'doi', 2294: 'táng', 2295: 'tâu', 2296: 'vượn', 2297: 'hót', 2298: 'nhăn', 2299: 'cãi', 2300: 'gán', 2301: 'nhẽ', 2302: 'nắm', 2303: 'níp', 2304: 'rặt', 2305: 'ổn', 2306: 'lượt', 2307: 'giáo', 2308: 'dẫy', 2309: 'thuỷ', 2310: 'dâm', 2311: 'thửa', 2312: 'khiên', 2313: 'thả', 2314: 'bè', 2315: 'chụm', 2316: 'chài', 2317: 'giăng', 2318: 'mui', 2319: 'lướt', 2320: 'mướt', 2321: 'dân', 2322: 'hưởng', 2323: 'trặn', 2324: 'lư', 2325: 'chay', 2326: 'quạnh', 2327: 'quẽ', 2328: 'ngoái', 2329: 'xập', 2330: 'xè', 2331: 'liệng', 2332: 'giềng', 2333: 'sút', 2334: 'tả', 2335: 'phên', 2336: 'chú', 2337: 'dỗ', 2338: 'chăm', 2339: 'tõi', 2340: 'nhe', 2341: 'nung', 2342: 'don', 2343: 'yểu', 2344: 'bai', 2345: 'muộn', 2346: 'chế', 2347: 'khoa', 2348: 'chiếm', 2349: 'bảng', 2350: 'chu', 2351: 'thê', 2352: 'thăng', 2353: 'kiên', 2354: 'tá', 2355: 'ngườị', 2356: 'mồng', 2357: 'giũ', 2358: 'ấn', 2359: 'lội', 2360: 'dấn', 2361: 'dờỉ', 2362: 'xảy', 2363: 'giặc', 2364: 'xúm', 2365: 'khỏe', 2366: 'quây', 2367: 'nếm', 2368: 'thỏa', 2369: 'vừạ', 2370: 'mềm', 2371: 'ao', 2372: 'lọc', 2373: 'sẻ', 2374: 'rằm', 2375: 'chối', 2376: 'bái', 2377: 'xướng', 2378: 'nhuốc', 2379: 'mò', 2380: 'đùm', 2381: 'bọc', 2382: 'nến', 2383: 'điệp', 2384: 'đế', 2385: 'điền', 2386: 'chí', 2387: 'bạn', 2388: 'trùm', 2389: 'ngạch', 2390: 'len', 2391: 'hái', 2392: 'nghàn', 2393: 'lẫn'}\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "# Tokenize the text data\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(textCorpus)\n",
        "total_words = len(tokenizer.word_index) + 1\n",
        "print(f'Total words in the corpus: {total_words}')\n",
        "print(tokenizer.index_word)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlbsFcou7k7e"
      },
      "source": [
        "# Prepare training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZArK4Xy77r9e"
      },
      "source": [
        "## Convert a sentence into a sequence of numbers\n",
        "\n",
        "With the vocabulary, a sentence can be converted into a sequence of numbers (the index of each word in the sentence), as the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShcoVSDM880S",
        "outputId": "cf7119ad-f831-4629-a2b7-146fa3df26b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trăm năm trong cõi người ta\n",
            "[140, 76, 26, 497, 3, 65]\n"
          ]
        }
      ],
      "source": [
        "firstSentence = textCorpus[0]\n",
        "sequence = tokenizer.texts_to_sequences([firstSentence])[0]\n",
        "print(firstSentence)\n",
        "print(sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Text to Sequence Conversion\n",
        "\n",
        "Here we see how a sentence becomes a sequence of numbers:\n",
        "- Each word gets mapped to its unique integer index from the vocabulary\n",
        "- For example: \"Trăm\" → 1, \"năm\" → 2, etc.\n",
        "- This numerical representation is what the LSTM model will process"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH7JffI-93cf"
      },
      "source": [
        "## Create n-gram sequences\n",
        "\n",
        "N-gram of texts are extensively used in text mining and natural language processing tasks.\n",
        "\n",
        "An n-gram is a sequence of n words: a 2-gram (which we'll call\n",
        "bigram) is a two-word sequence of words like: \"Trăm năm\", or \"năm trong\", and a 3-gram (a trigram) is a three-word sequence of words like: \"Trăm năm trong\" or \"năm trong cõi\".\n",
        "\n",
        "**Why n-grams matter:** When generating text, the model predicts the next word based on previous words. N-grams teach the model different context lengths - 2-grams (\"Trăm năm\") help learn immediate word pairs, while longer n-grams help learn more complex phrases.\n",
        "\n",
        "To create the training data, from each sentence in our corpus, we will create 2-gram, 3-gram, ... sequences from left to right. For example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSuAGqDgBUaK",
        "outputId": "fef2194c-adea-4819-9730-7764a1572204"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[140, 76]\n",
            "[140, 76, 26]\n",
            "[140, 76, 26, 497]\n",
            "[140, 76, 26, 497, 3]\n",
            "[140, 76, 26, 497, 3, 65]\n"
          ]
        }
      ],
      "source": [
        "# Create n-gram sequences for the first sentence\n",
        "for i in range(1, len(sequence)):\n",
        "    n_gram_sequence = sequence[:i+1]\n",
        "    print(n_gram_sequence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IX0kQ04BB6nF"
      },
      "source": [
        "## Create input sequences\n",
        "\n",
        "Now, we will create a set of all n-gram sequences of all sentences in the corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAIa4AkJNlWo",
        "outputId": "6a092ee4-1a35-487a-dd86-472f488d4569"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The number of sequences: 19549\n",
            "[[140, 76], [140, 76, 26], [140, 76, 26, 497], [140, 76, 26, 497, 3], [140, 76, 26, 497, 3, 65], [126, 118], [126, 118, 126], [126, 118, 126, 374], [126, 118, 126, 374, 351], [126, 118, 126, 374, 351, 8], [126, 118, 126, 374, 351, 8, 1173], [126, 118, 126, 374, 351, 8, 1173, 82], [1174, 194], [1174, 194, 1], [1174, 194, 1, 673], [1174, 194, 1, 673, 184], [1174, 194, 1, 673, 184, 604], [53, 94], [53, 94, 61], [53, 94, 61, 49]]\n",
            "['trăm năm']\n",
            "['trăm năm trong']\n",
            "['trăm năm trong cõi']\n",
            "['trăm năm trong cõi người']\n",
            "['trăm năm trong cõi người ta']\n",
            "['chữ tài']\n",
            "['chữ tài chữ']\n",
            "['chữ tài chữ mệnh']\n",
            "['chữ tài chữ mệnh khéo']\n",
            "['chữ tài chữ mệnh khéo là']\n"
          ]
        }
      ],
      "source": [
        "# Create a set of all n-gram sequences\n",
        "sequences = []\n",
        "for sentence in textCorpus:\n",
        "    sequence = tokenizer.texts_to_sequences([sentence])[0]\n",
        "    for i in range(1, len(sequence)):\n",
        "        n_gram_sequence = sequence[:i+1]\n",
        "        sequences.append(n_gram_sequence)\n",
        "print(f'The number of sequences: {len(sequences)}')\n",
        "print(sequences[:20])\n",
        "\n",
        "# Print out the first tenth n-gram text\n",
        "for i in range(10):\n",
        "    print(tokenizer.sequences_to_texts([sequences[i]]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Generate All N-gram Sequences\n",
        "\n",
        "**Process:**\n",
        "1. Convert each sentence to a sequence of word indices\n",
        "2. Create all possible n-grams (2-gram, 3-gram, ..., up to full sentence)\n",
        "3. Store all sequences in a list\n",
        "\n",
        "**Result:** We have ~19,549 training examples, each showing the model a partial sentence and teaching it what word comes next. This comprehensive dataset helps the model learn word patterns across different context lengths."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t94MYhMGEd2N"
      },
      "source": [
        "## Create training data\n",
        "\n",
        "We have `19,549` sequences in our training data. However, the lengths of the sequences are not equal.\n",
        "\n",
        "**Why padding?** Neural networks require all inputs in a batch to have the same shape. Since our sequences have different lengths, we pad them with zeros at the beginning. This makes batch processing efficient during training - all sequences become uniform length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnFJCWC4O7-Q",
        "outputId": "ee84fd45-c796-49f8-b5f1-084e80ff171c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(19549, 14)\n",
            "[  0   0   0   0   0   0   0   0   0   0   0   0 140  76]\n",
            "[  0   0   0   0   0   0   0   0   0   0   0 140  76  26]\n",
            "[  0   0   0   0   0   0   0   0   0   0 140  76  26 497]\n",
            "[  0   0   0   0   0   0   0   0   0 140  76  26 497   3]\n",
            "[  0   0   0   0   0   0   0   0 140  76  26 497   3  65]\n"
          ]
        }
      ],
      "source": [
        "# Pad sequences\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "max_sequence_len = max([len(seq) for seq in sequences])\n",
        "sequences = np.array(pad_sequences(sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "print(sequences.shape)\n",
        "for i in range(5):\n",
        "    print(sequences[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eBWw8lvGaqK"
      },
      "source": [
        "We consider the last element of each sequence to be a label. This means that our model will predict the label (the index of the last word in the sentence) based on the first thirteen elements.\n",
        "\n",
        "**One-hot encoding:** We convert word indices to one-hot vectors (e.g., word index 5 becomes [0,0,0,0,0,1,0,...,0]). This is needed because we're doing **multi-class classification** - predicting which word from the vocabulary (2394 possible words) comes next.\n",
        "\n",
        "Now we can create the input features and output labels for our training data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pt_lKVmHMVOi",
        "outputId": "7f273219-6270-4147-9a9c-f3f9393badd3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[  0   0   0   0   0   0   0   0   0   0   0   0 140] --> 76\n",
            "[  0   0   0   0   0   0   0   0   0   0   0 140  76] --> 26\n",
            "[  0   0   0   0   0   0   0   0   0   0 140  76  26] --> 497\n",
            "[  0   0   0   0   0   0   0   0   0 140  76  26 497] --> 3\n",
            "[  0   0   0   0   0   0   0   0 140  76  26 497   3] --> 65\n",
            "(19549, 13)\n",
            "(19549, 2394)\n"
          ]
        }
      ],
      "source": [
        "X, Y = sequences[:, :-1], sequences[:, -1]\n",
        "for i in range(5):\n",
        "  print(X[i], '-->', Y[i])\n",
        "\n",
        "# Convert target data to one-hot encoding\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "Y = to_categorical(Y, num_classes = total_words)\n",
        "\n",
        "print(X.shape)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create Training Data (X and Y)\n",
        "\n",
        "**Splitting features and labels:**\n",
        "- `X`: All words except the last one (input context)\n",
        "- `Y`: The last word (what we want to predict)\n",
        "\n",
        "**One-hot encoding Y:**\n",
        "- Converts word indices to binary vectors\n",
        "- Example: Word index 5 becomes [0,0,0,0,0,1,0,...,0]\n",
        "- Necessary for multi-class classification (choosing from 2394 possible words)\n",
        "\n",
        "**Final shapes:**\n",
        "- X: (19549, 13) - 19549 sequences, each up to 13 words long\n",
        "- Y: (19549, 2394) - 19549 labels, each a one-hot vector of vocabulary size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhOLSx4JIPpq"
      },
      "source": [
        "# Specify and train a LSTM model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "LkiI5k5zK9Cl"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "def create_LSTM(vocab_size):\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim = vocab_size, output_dim = 256))\n",
        "  model.add(LSTM(units = 256))\n",
        "  model.add(Dense(units = vocab_size, activation = 'softmax'))\n",
        "  return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LSTM Model Architecture\n",
        "\n",
        "**Layer breakdown:**\n",
        "1. **Embedding Layer**: Converts word indices to dense 256-dimensional vectors\n",
        "   - Maps each word to a learned representation that captures semantic meaning\n",
        "   - Reduces dimensionality and helps the model understand word relationships\n",
        "\n",
        "2. **LSTM Layer (256 units)**: The core sequential processor\n",
        "   - Maintains long-term memory through gates (input, forget, output)\n",
        "   - Learns which words are important for predicting the next word\n",
        "   - Better than SimpleRNN at remembering long-range dependencies\n",
        "\n",
        "3. **Dense Output Layer**: Produces probability distribution over vocabulary\n",
        "   - `softmax` activation: Converts outputs to probabilities (sum = 1)\n",
        "   - Each output represents the probability of each word being next\n",
        "\n",
        "**Why LSTM for text generation?**\n",
        "- Handles variable-length sequences\n",
        "- Remembers important context from earlier in the sentence\n",
        "- Can learn complex patterns like grammar and word associations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP0h9TPHLBYT",
        "outputId": "67134ba5-f00c-4431-97e3-a30e8148a72c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.0110 - loss: 7.1691\n",
            "Epoch 2/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.0257 - loss: 6.4948\n",
            "Epoch 3/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.0408 - loss: 6.0309\n",
            "Epoch 4/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.0693 - loss: 5.5138\n",
            "Epoch 5/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.1310 - loss: 4.8639\n",
            "Epoch 6/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.2121 - loss: 4.2090\n",
            "Epoch 7/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.3145 - loss: 3.5328\n",
            "Epoch 8/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.4365 - loss: 2.9113\n",
            "Epoch 9/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.5393 - loss: 2.4037\n",
            "Epoch 10/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.6280 - loss: 1.9541\n",
            "Epoch 11/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.6960 - loss: 1.5939\n",
            "Epoch 12/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.7544 - loss: 1.2864\n",
            "Epoch 13/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.7859 - loss: 1.0715\n",
            "Epoch 14/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8141 - loss: 0.8865\n",
            "Epoch 15/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8297 - loss: 0.7656\n",
            "Epoch 16/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8343 - loss: 0.6850\n",
            "Epoch 17/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8451 - loss: 0.6142\n",
            "Epoch 18/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8453 - loss: 0.5808\n",
            "Epoch 19/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8441 - loss: 0.5504\n",
            "Epoch 20/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8440 - loss: 0.5253\n",
            "Epoch 21/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8479 - loss: 0.5020\n",
            "Epoch 22/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8505 - loss: 0.4827\n",
            "Epoch 23/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8405 - loss: 0.4952\n",
            "Epoch 24/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8434 - loss: 0.4856\n",
            "Epoch 25/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8480 - loss: 0.4668\n",
            "Epoch 26/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8458 - loss: 0.4582\n",
            "Epoch 27/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8475 - loss: 0.4495\n",
            "Epoch 28/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8504 - loss: 0.4438\n",
            "Epoch 29/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8481 - loss: 0.4382\n",
            "Epoch 30/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8472 - loss: 0.4365\n",
            "Epoch 31/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8518 - loss: 0.4217\n",
            "Epoch 32/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8457 - loss: 0.4375\n",
            "Epoch 33/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8481 - loss: 0.4257\n",
            "Epoch 34/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8472 - loss: 0.4260\n",
            "Epoch 35/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8461 - loss: 0.4250\n",
            "Epoch 36/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8459 - loss: 0.4264\n",
            "Epoch 37/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8481 - loss: 0.4145\n",
            "Epoch 38/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8507 - loss: 0.4068\n",
            "Epoch 39/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8514 - loss: 0.4096\n",
            "Epoch 40/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8447 - loss: 0.4186\n",
            "Epoch 41/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8483 - loss: 0.4060\n",
            "Epoch 42/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8482 - loss: 0.4109\n",
            "Epoch 43/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8493 - loss: 0.3990\n",
            "Epoch 44/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8457 - loss: 0.4077\n",
            "Epoch 45/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8452 - loss: 0.4070\n",
            "Epoch 46/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8481 - loss: 0.3991\n",
            "Epoch 47/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - accuracy: 0.8519 - loss: 0.3907\n",
            "Epoch 48/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8450 - loss: 0.4081\n",
            "Epoch 49/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8463 - loss: 0.4060\n",
            "Epoch 50/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8500 - loss: 0.3935\n",
            "Epoch 51/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8508 - loss: 0.3925\n",
            "Epoch 52/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8469 - loss: 0.4056\n",
            "Epoch 53/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8440 - loss: 0.4005\n",
            "Epoch 54/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8438 - loss: 0.4008\n",
            "Epoch 55/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8505 - loss: 0.3890\n",
            "Epoch 56/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8456 - loss: 0.3998\n",
            "Epoch 57/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8480 - loss: 0.3898\n",
            "Epoch 58/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8473 - loss: 0.3945\n",
            "Epoch 59/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8470 - loss: 0.3925\n",
            "Epoch 60/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8527 - loss: 0.3811\n",
            "Epoch 61/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8534 - loss: 0.3788\n",
            "Epoch 62/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8456 - loss: 0.3906\n",
            "Epoch 63/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8437 - loss: 0.3991\n",
            "Epoch 64/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8533 - loss: 0.3753\n",
            "Epoch 65/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8505 - loss: 0.3793\n",
            "Epoch 66/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8486 - loss: 0.3969\n",
            "Epoch 67/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8477 - loss: 0.3920\n",
            "Epoch 68/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8516 - loss: 0.3747\n",
            "Epoch 69/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8500 - loss: 0.3813\n",
            "Epoch 70/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8506 - loss: 0.3778\n",
            "Epoch 71/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8488 - loss: 0.3789\n",
            "Epoch 72/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8476 - loss: 0.3882\n",
            "Epoch 73/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8440 - loss: 0.3856\n",
            "Epoch 74/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8492 - loss: 0.3771\n",
            "Epoch 75/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8473 - loss: 0.3866\n",
            "Epoch 76/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8455 - loss: 0.3873\n",
            "Epoch 77/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8499 - loss: 0.3798\n",
            "Epoch 78/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8468 - loss: 0.3865\n",
            "Epoch 79/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8505 - loss: 0.3758\n",
            "Epoch 80/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8559 - loss: 0.3651\n",
            "Epoch 81/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8486 - loss: 0.3892\n",
            "Epoch 82/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8500 - loss: 0.3841\n",
            "Epoch 83/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8489 - loss: 0.3808\n",
            "Epoch 84/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8467 - loss: 0.3784\n",
            "Epoch 85/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8519 - loss: 0.3686\n",
            "Epoch 86/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8537 - loss: 0.3668\n",
            "Epoch 87/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8515 - loss: 0.3684\n",
            "Epoch 88/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8466 - loss: 0.3842\n",
            "Epoch 89/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8510 - loss: 0.3733\n",
            "Epoch 90/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8461 - loss: 0.3907\n",
            "Epoch 91/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8495 - loss: 0.3748\n",
            "Epoch 92/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8511 - loss: 0.3692\n",
            "Epoch 93/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8482 - loss: 0.3804\n",
            "Epoch 94/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8510 - loss: 0.3714\n",
            "Epoch 95/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 7ms/step - accuracy: 0.8510 - loss: 0.3693\n",
            "Epoch 96/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 8ms/step - accuracy: 0.8517 - loss: 0.3741\n",
            "Epoch 97/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8460 - loss: 0.3838\n",
            "Epoch 98/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8457 - loss: 0.3914\n",
            "Epoch 99/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.8446 - loss: 0.3794\n",
            "Epoch 100/100\n",
            "\u001b[1m611/611\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8474 - loss: 0.3730\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x79d2d896cfb0>"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Specify, compile and train the model\n",
        "model = create_LSTM(total_words)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, Y, epochs=100, verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Training Configuration\n",
        "\n",
        "**Compilation settings:**\n",
        "- `loss='categorical_crossentropy'`: Standard loss for multi-class classification\n",
        "  - Measures how different predictions are from actual next words\n",
        "- `optimizer='adam'`: Adaptive learning rate optimizer\n",
        "  - Efficiently updates model weights during training\n",
        "- `metrics=['accuracy']`: Tracks how often model predicts the correct next word\n",
        "\n",
        "**Training:**\n",
        "- `epochs=100`: Model sees all training data 100 times\n",
        "- Higher epochs help model learn better patterns but risk overfitting\n",
        "- Monitor accuracy - it should increase as training progresses\n",
        "\n",
        "⚠️ **Note:** Training 100 epochs may take considerable time depending on your hardware!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "Qln11Ur1S96i",
        "outputId": "cfb21aca-bf5b-461c-86bb-eda168eb052b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)        │       <span style=\"color: #00af00; text-decoration-color: #00af00\">612,864</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">525,312</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2394</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">615,258</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m256\u001b[0m)        │       \u001b[38;5;34m612,864\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m525,312\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2394\u001b[0m)           │       \u001b[38;5;34m615,258\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,260,304</span> (20.07 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,260,304\u001b[0m (20.07 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,753,434</span> (6.69 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,753,434\u001b[0m (6.69 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,506,870</span> (13.38 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m3,506,870\u001b[0m (13.38 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "tLBeVyBKRnr0"
      },
      "outputs": [],
      "source": [
        "model.save('lstm_word_generation_v1.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Save the Trained Model\n",
        "\n",
        "Saving the model allows us to:\n",
        "- Use it later without retraining (saves hours!)\n",
        "- Share the trained model with others\n",
        "- Deploy it in applications\n",
        "\n",
        "The `.keras` format is TensorFlow's modern model format."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6cP3Hu6OIQW"
      },
      "source": [
        "# Test the model\n",
        "\n",
        "A pre-trained model can be downloaded from the following link:\n",
        "\n",
        "[https://drive.google.com/file/d/1T6jYBTB6HGke4dyPQhGqoeIlSD_LKUBK/view?usp=sharing](https://drive.google.com/file/d/1T6jYBTB6HGke4dyPQhGqoeIlSD_LKUBK/view?usp=sharing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "-AZMU9wLSR2z"
      },
      "outputs": [],
      "source": [
        "# Load the pre-trained model\n",
        "\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "model = load_model('lstm_word_generation_v1.keras')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load Pre-trained Model\n",
        "\n",
        "Instead of training from scratch (which takes a long time), we can load a pre-trained model. This is especially useful for:\n",
        "- Testing and experimentation\n",
        "- When you don't have time/resources to train\n",
        "- Demonstrations and tutorials"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2X-SgL_7TCod",
        "outputId": "0244c971-fb4b-4914-abf6-c2d6aecf268e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 174ms/step\n",
            "phận mình thì\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "phận mình thì cũng\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "phận mình thì cũng có\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "phận mình thì cũng có nhà\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "phận mình thì cũng có nhà cũng\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "phận mình thì cũng có nhà cũng không\n",
            "Next predicted words: phận mình thì cũng có nhà cũng không\n"
          ]
        }
      ],
      "source": [
        "# Generate next word predictions\n",
        "current_text = \"phận mình\"\n",
        "next_words = 6\n",
        "\n",
        "for i in range(next_words):\n",
        "    sequence = tokenizer.texts_to_sequences([current_text])[0]\n",
        "    sequence = pad_sequences([sequence], maxlen=max_sequence_len-1, padding='pre')\n",
        "    predicted_probs = model.predict(sequence)\n",
        "    predicted_word = tokenizer.index_word[np.argmax(predicted_probs)]\n",
        "    current_text += \" \" + predicted_word\n",
        "    print(current_text)\n",
        "\n",
        "print(\"Next predicted words:\", current_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Text Generation Process\n",
        "\n",
        "**How it works (iteratively):**\n",
        "1. Start with seed text: \"phận mình\"\n",
        "2. Convert current text to sequence of word indices\n",
        "3. Pad sequence to match training data length\n",
        "4. Model predicts probability distribution over all vocabulary words\n",
        "5. Select word with highest probability (argmax)\n",
        "6. Append predicted word to current text\n",
        "7. Repeat steps 2-6 for desired number of words\n",
        "\n",
        "**Why this approach?**\n",
        "- Generates text word-by-word like humans write\n",
        "- Each prediction builds on previous context\n",
        "- Creates coherent, contextually-appropriate text\n",
        "\n",
        "The model continues the phrase in the style of \"Truyện Kiều\"!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
