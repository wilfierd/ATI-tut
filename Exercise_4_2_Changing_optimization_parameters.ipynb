{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4.2 Changing optimization parameters\n"
      ],
      "metadata": {
        "id": "ApEGsGnec6BZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you will use the same titanic dataset from the previous exercises. You can download the dataset from the following link:\n",
        "\n",
        "[titanic_all_numeric.csv](https://drive.google.com/file/d/11nuYS-l3EXCsGJt81y4YTt3oTnFGaB68/view?usp=drive_link)\n",
        "\n",
        "The data is pre-loaded into a pandas DataFrame called `df`. The `predictors` and `target` values are also pre-defined.\n",
        "\n",
        "You'll want the optimization to start from scratch every time you change the learning rate, to give a fair comparison of how each learning rate did in your results. So we have created a function `get_new_model()` that creates an unoptimized model to optimize.\n",
        "\n",
        "It's time to get your hands dirty with optimization. You'll now try optimizing a model at a very low learning rate, a very high learning rate, and a \"just right\" learning rate. You'll want to look at the results after running this exercise, remembering that a low value for the loss function is good."
      ],
      "metadata": {
        "id": "mEOOH6NdV42q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "-AWPd5Wqft_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Import `SGD` from `tensorflow.keras.optimizers`.\n",
        "* Create a list of learning rates to try optimizing with called `lr_to_test`. The learning rates in it should be `0.000001`, `0.01`, and `1.0`.\n",
        "* Using a `for` loop to iterate over `lr_to_test`:\n",
        "  * Use the `get_new_model()` function to build a new, unoptimized model.\n",
        "  * Create an optimizer called `my_optimizer` using the `SGD()` constructor with keyword argument `learning_rate=lr`.\n",
        "  * Compile your model. Set the optimizer parameter to be the SGD object you created above, and because this is a classification problem, use `'categorical_crossentropy'` for the loss parameter, , and `metrics=['accuracy']` to see the accuracy at the end of each epoch.\n",
        "  * Fit the model using the `predictors` and the `target`."
      ],
      "metadata": {
        "id": "fNotC4abWBqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "oKx-pAyrlVGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data and convert the data to NumPy array:"
      ],
      "metadata": {
        "id": "K2uQ5t2BRtpg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dlu9wWcWcy44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb611207-d335-4c89-8cb8-a0ee61321d1a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-972366677.py:11: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
            "  df.age_was_missing = df.age_was_missing.replace({True: 1, False: 0})\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Load csv file from Google Drive into the dataframe: df\n",
        "# Using the direct download link for the CSV file from your Google Drive\n",
        "url = \"https://drive.google.com/uc?export=download&id=11nuYS-l3EXCsGJt81y4YTt3oTnFGaB68\"\n",
        "df = pd.read_csv(url)\n",
        "\n",
        "# Convert the boolean values of the 'age_was_missing' column to integer\n",
        "df.age_was_missing = df.age_was_missing.replace({True: 1, False: 0})\n",
        "\n",
        "# Create predictors NumPy array: predictors\n",
        "predictors = df.drop(['survived'], axis=1).values\n",
        "\n",
        "# Save the number of columns in predictors: n_cols\n",
        "n_cols = predictors.shape[1]\n",
        "\n",
        "# Convert the target to categorical: target\n",
        "target = to_categorical(df['survived'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a neural network for a classification task"
      ],
      "metadata": {
        "id": "9jf6A0pPsATU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "def get_new_model():\n",
        "  # Set up the model\n",
        "  model = Sequential()\n",
        "  model.add(Dense(32, activation='relu', input_shape=(n_cols,)))\n",
        "  model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "wkgdNpMmsKmt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Optimize the model with different learning rate:"
      ],
      "metadata": {
        "id": "u986qJ_NOK1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the SGD optimizer\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Create list of learning rates: lr_to_test\n",
        "lr_to_test = [0.000001, 0.01, 1.0]\n",
        "\n",
        "# Loop over learning rates\n",
        "for lr in lr_to_test:\n",
        "    print('\\n\\nTesting model with learning rate: %f\\n'%lr )\n",
        "\n",
        "    # Build new model to test, unaffected by previous models\n",
        "    model = get_new_model()\n",
        "\n",
        "    # Create SGD optimizer with specified learning rate: my_optimizer\n",
        "    my_optimizer = SGD(learning_rate=lr)\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer=my_optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # Fit the model\n",
        "    model.fit(predictors, target)"
      ],
      "metadata": {
        "id": "rdrPE2NrOjS5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9de2b3d8-184d-40f8-d890-f4839293fc93"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Testing model with learning rate: 0.000001\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6536 - loss: 3.9232   \n",
            "\n",
            "\n",
            "Testing model with learning rate: 0.010000\n",
            "\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6120 - loss: 2.4659   \n",
            "\n",
            "\n",
            "Testing model with learning rate: 1.000000\n",
            "\n",
            "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6136 - loss: 581.0734 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ouput should be:\n",
        "\n",
        "Testing model with learning rate: 0.000001\n",
        "\n",
        "28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.4074 - loss: 5.5751   \n",
        "\n",
        "\n",
        "Testing model with learning rate: 0.010000\n",
        "\n",
        "28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.5747 - loss: 3.3378   \n",
        "\n",
        "\n",
        "Testing model with learning rate: 1.000000\n",
        "\n",
        "28/28 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step - accuracy: 0.5404 - loss: 28697.6289\n"
      ],
      "metadata": {
        "id": "ecubSk7GZKKp"
      }
    }
  ]
}