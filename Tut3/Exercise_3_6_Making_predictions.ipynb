{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "ApEGsGnec6BZ",
        "-AWPd5Wqft_d"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3.6 Making predictions\n"
      ],
      "metadata": {
        "id": "ApEGsGnec6BZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, you will write code to use the trained model to make predictions on new data.\n",
        "\n",
        "You can download the titanic dataset from the following link:\n",
        "\n",
        "[titanic_all_numeric.csv](https://drive.google.com/file/d/11nuYS-l3EXCsGJt81y4YTt3oTnFGaB68/view?usp=drive_link)\n",
        "\n",
        "The data is pre-loaded into a pandas DataFrame called `df`. We will divide our data into two subsets: the first one (800 rows) for training and the second one (91 rows) for predictions using the trained model.\n",
        "\n",
        "The trained network from your previous coding exercise is now stored as model. New data to make predictions is stored in a NumPy array as `pred_data`. Use model to make predictions on your new data.\n",
        "\n",
        "In this exercise, your predictions will be probabilities, which is the most common way for data scientists to communicate their predictions to colleagues."
      ],
      "metadata": {
        "id": "mEOOH6NdV42q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Instructions"
      ],
      "metadata": {
        "id": "-AWPd5Wqft_d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Create your predictions using the model's `predict()` method on `pred_data`.\n",
        "* Use NumPy indexing to find the column corresponding to predicted probabilities of survival being `True`. This is the second column (index `1`) of `predictions`. Store the result in `predicted_prob_true` and print it."
      ],
      "metadata": {
        "id": "fNotC4abWBqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Code"
      ],
      "metadata": {
        "id": "oKx-pAyrlVGO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load data and convert the data to NumPy array:"
      ],
      "metadata": {
        "id": "K2uQ5t2BRtpg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlu9wWcWcy44"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load csv file into the dataframe: df\n",
        "df = pd.read_csv(\"titanic_all_numeric.csv\")\n",
        "\n",
        "# Convert the boolean values of the 'age_was_missing' column to integer\n",
        "df.age_was_missing = df.age_was_missing.replace({True: 1, False: 0})\n",
        "\n",
        "# The dataframe df has 891 rows, we will divide df into two parts\n",
        "# The first 800 rows are used to create the predictors for training the model\n",
        "# Other 91 rows are used to create the pred_data for making predictions with the model\n",
        "trainDF = df.iloc[:800,:]\n",
        "predictDF = df.iloc[800:,:]\n",
        "print(df.shape)\n",
        "print(trainDF.shape)\n",
        "print(predictDF.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Create predictors NumPy array: predictors\n",
        "predictors = trainDF.drop(['survived'], axis=1).values\n",
        "\n",
        "# Save the number of columns in predictors: n_cols\n",
        "n_cols = predictors.shape[1]\n",
        "\n",
        "# Convert the target to categorical: target\n",
        "target = to_categorical(trainDF['survived'])\n",
        "\n",
        "# Create data for predictions NumPy array: pred_data\n",
        "pred_data = predictDF.drop(['survived'], axis=1).values\n"
      ],
      "metadata": {
        "id": "ZNXWyQam59OX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the neural network, then compile and fit the model"
      ],
      "metadata": {
        "id": "u986qJ_NOK1h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify, compile, and fit the model\n",
        "model = Sequential()\n",
        "model.add(Dense(32, activation='relu', input_shape = (n_cols,)))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "model.compile(optimizer='sgd',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(predictors, target)\n"
      ],
      "metadata": {
        "id": "rdrPE2NrOjS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create the predictions using the trained model"
      ],
      "metadata": {
        "id": "3_zqKZ4p8Jlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate predictions: predictions\n",
        "predictions = ____\n",
        "\n",
        "# Calculate predicted probability of survival: predicted_prob_true\n",
        "predicted_prob_true = ____\n",
        "\n",
        "# Print predicted_prob_true\n",
        "print(predicted_prob_true)"
      ],
      "metadata": {
        "id": "w7epPF2A8VYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ouput should be:\n",
        "\n",
        "3/3 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step\n",
        "\n",
        "[0.67805415 0.67400146 0.9999853  0.3875375  0.5985795  0.6164698\n",
        " 0.89714897 0.5290434  0.694053   0.883478   0.5992863  0.7111521\n",
        " 0.670607   0.6320936  0.6159816  0.85379726 0.54823047 0.7421135\n",
        " 0.6533071  0.54122615 0.97659034 0.60646343 0.8911979  0.6048734\n",
        " 0.88516706 0.62198186 0.91774607 0.9476062  0.62564427 0.93983597\n",
        " 0.50025886 0.71690863 0.59585077 0.58837867 0.5724051  0.9671306\n",
        " 0.5849963  0.61918813 0.9129218  0.784304   0.57787883 0.6095583\n",
        " 0.75754553 0.60315263 0.5694081  0.65394723 0.7943384  0.6117081\n",
        " 0.77466637 0.9892311  0.6483037  0.77982235 0.47878534 0.8670235\n",
        " 0.6792838  0.55752456 0.999963   0.79640836 0.6378785  0.59585077\n",
        " 0.588558   0.58213407 0.7576602  0.80326736 0.6440964  0.6678745\n",
        " 0.5529083  0.89298415 0.62583524 0.46835485 0.59932536 0.8583759\n",
        " 0.6688276  0.67672044 0.6243774  0.49477854 0.5868015  0.5741109\n",
        " 0.61847866 0.9318947  0.7129401  0.6240244  0.55705667 0.6472381\n",
        " 0.591772   0.7761354  0.6546704  0.77318966 0.6351063  0.7619014\n",
        " 0.62768227]\n"
      ],
      "metadata": {
        "id": "ecubSk7GZKKp"
      }
    }
  ]
}